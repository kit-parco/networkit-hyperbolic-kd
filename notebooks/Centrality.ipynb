{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Centrality\n",
      "==========\n",
      "\n",
      "This evaluates the Eigenvector Centrality and PageRank implemented in Python against C++-native EVZ and PageRank. The Python implementation uses SciPy (and thus ARPACK) to compute the eigenvectors, while the C++ method implements a power iteration method itself."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import NetworKit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Update to Python >=3.4 recommended - support for < 3.4 may be discontinued in the future\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "G = NetworKit.graphio.readGraph(\"../input/celegans_metabolic.graph\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we just compute the Python EVZ and display a sample. The \"scores()\" method returns a list of centrality scores in order of the vertices. Thus, what you see below are the (normalized, see the respective argument) centrality scores for G.nodes()[0], G.nodes()[1], ... "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evzNative = NetworKit.centrality.PythonNativeEVZ(G, normalized=True)\n",
      "evzNative.run()\n",
      "evzNative.scores()[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[0.038890947542671417,\n",
        " 0.025204387816348928,\n",
        " 0.030099862584687796,\n",
        " 0.023008066197884643,\n",
        " 0.015014306929265246,\n",
        " 0.047450338825435673,\n",
        " 0.03877379800819393,\n",
        " 0.00018201165419890557,\n",
        " 0.0043027361546468185,\n",
        " 0.019820501996514393]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now take a look at the 10 most central vertices according to the four heuristics. Here, the centrality algorithms offer the ranking() method that returns a list of (vertex, centrality) ordered by centrality."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evzNative.ranking()[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "[(185, 0.37998920394223168),\n",
        " (146, 0.25589844844192655),\n",
        " (407, 0.25299992743917177),\n",
        " (144, 0.21952034822779035),\n",
        " (204, 0.18014173124747021),\n",
        " (230, 0.17037022146635697),\n",
        " (227, 0.15343976623211927),\n",
        " (226, 0.15343976623211925),\n",
        " (152, 0.15256017345576342),\n",
        " (425, 0.13620265458341316)]"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compute the EVZ using the C++ backend and also display the 10 most important vertices, just as above. This should hopefully look similar...\n",
      "\n",
      "*Please note*: The normalization argument may not be passed as a named argument to the C++-backed centrality measures. This is due to some limitation in the C++ wrapping code."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evz = NetworKit.centrality.EigenvectorCentrality(G, True)\n",
      "evz.run()\n",
      "evz.ranking()[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[(185, 0.3586787763956071),\n",
        " (407, 0.2420807643210619),\n",
        " (146, 0.23956989544776153),\n",
        " (144, 0.20553600559712537),\n",
        " (204, 0.177385111798723),\n",
        " (230, 0.16792666595381617),\n",
        " (152, 0.1548329695474894),\n",
        " (227, 0.1491114355708722),\n",
        " (226, 0.1491114355708722),\n",
        " (425, 0.13360064298606755)]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, let's take a look at the PageRank. First, compute the PageRank using the C++ backend and display the 10 most important vertices. The second argument to the algorithm is the dampening factor, i.e. the probability that a random walk just stops at a vertex and instead teleports to some other vertex."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pageRank = NetworKit.centrality.PageRank(G, 0.95, True)\n",
      "pageRank.run()\n",
      "pageRank.ranking()[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[(185, 0.08967935808573081),\n",
        " (146, 0.03629366353976929),\n",
        " (144, 0.031391049006808995),\n",
        " (407, 0.029966678067660842),\n",
        " (227, 0.02119029305575018),\n",
        " (226, 0.02119029305575018),\n",
        " (425, 0.01990720675472069),\n",
        " (152, 0.017614662527267698),\n",
        " (230, 0.016888616220117534),\n",
        " (228, 0.016241705781972086)]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Same in Python..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nativePageRank = NetworKit.centrality.PythonNativePageRank(G, 0.95, normalized=True)\n",
      "nativePageRank.run()\n",
      "nativePageRank.ranking()[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "[(185, 0.57969868523261647),\n",
        " (146, 0.23066656271510738),\n",
        " (351, 0.21628413395626683),\n",
        " (407, 0.21460542506626207),\n",
        " (144, 0.18890292106322537),\n",
        " (227, 0.1864947362518129),\n",
        " (226, 0.18365293338896607),\n",
        " (154, 0.17205038099379336),\n",
        " (152, 0.14806676688982734),\n",
        " (425, 0.13573514873786399)]"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If everything went well, these should look similar, too. You might notice a rather large constant factor between the centrality values. This seems to be because of a bug in the normalization of the C++ backend. Let's take a look at the magnitudes of the vectors (which should be 1 after normalization):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mag = lambda x: math.sqrt(sum((i[1])**2 for i in x))\n",
      "print(\"C++ magnitude: {}\".format(mag(pageRank.ranking())))\n",
      "print(\"Python magnitude: {}\".format(mag(nativePageRank.ranking())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C++ magnitude: 0.12586531423535924\n",
        "Python magnitude: 1.0000000000000002\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we take a look at the relative differences between the computed centralities for the vertices:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "differences = [(max(x[0], x[1]) / min(x[0], x[1])) - 1 for x in zip(evz.scores(), evzNative.scores())]\n",
      "print(\"Average relative difference: {}\".format(sum(differences) / len(differences)))\n",
      "print(\"Maximum relative difference: {}\".format(max(differences)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Average relative difference: 0.08411651551542425\n",
        "Maximum relative difference: 1.355588906163185\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}